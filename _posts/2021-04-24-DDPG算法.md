---
title: DDPG算法
layout: post
categories: reinforcement-learning
tags: reinforcement-learning DDPG
date: 2021-04-24 22:00
excerpt: DDPG算法
---

{:.table-of-content}
* TOC
{:toc}

# DDPG算法

DPG是一个比较特殊的算法，虽然它通常被归为PG算法族，但是显然它依据的根本还是Q-learning的思想。它跟普通Q-learning算法的区别是，它针对于连续动作空间。Q-learning算法族在选择动作时，都遍历计算给定状态下所有动作的Q值，然后选择Q值最大的那个动作。而这只对于离散动作空间有效，在动作个数为无限的连续动作空间中，是没有办法遍历计算所有动作的Q值函数的。因此**DPG学习两个函数，一个是Q值函数，一个是策略函数**，在选择动作时，由策略函数直接输出概率最大的那个动作（PG算法族中，策略函数输出的都是动作的概率分布）。这个由策略函数输出的动作同时也是使得学习到的Q值函数达到给定状态下最大Q值的动作，而在其他PG算法中，并不存在一个所谓的Q值函数，所以也不存在使得Q值函数最大的动作，存在的只是一个由Q值和其他参数组成的优化目标函数（目标函数中的Q值是通过Replay Buffer中的Transitions近似而成并作为真实值来使用的）。所以普通意义上的PG算法即为，学习一个策略函数，这个策略函数使得目标函数的值最大（即用策略函数（策略函数中的参数）作为一个approximator来逼近目标函数的最大值），这两个函数某种意义上是共享参数的，PG算法在training时只学习一组参数，即策略参数。而Q-learning算法是学习Q值函数的参数，**而特殊的这个DPG算法是学习两组参数，Q值函数的参数和策略函数的参数**。在对Q值函数进行对于策略参数的梯度计算时，将Q值函数的参数视为常数。下面来详细叙述DPG算法。

## Background

DDPG算法的全称是Deep Deterministic Policy Gradient。为什么是deterministic的呢？因为PG算法族学习到的策略函数常常输出一个关于动作的概率分布，而DDPG算法直接由策略函数输出一个动作，这个动作使得Q值函数的Q值最大。在Q-learning-based算法中，我们希望选择这样一个动作：
$$
a^*(s)=\mathop{arg\ max}\limits_{a} Q^*(s,a)
$$
这对于具有离散动作空间的环境很适用，因为它本质上是将给定状态下的所有动作的Q值计算出来，选择那个Q值最大的动作。但这对于具有连续动作空间的环境则不可能成功。DDPG算法不像Q-learning算法，只学习$$Q^*(s,a)$$的approximator，也不像PG算法，只学习$$\pi^*(s)$$的approximator，它同时学习$$Q^*(s,a)$$和$$\pi^*(s)$$的approximator。

**对于DDPG，我们具有这样一个前提：由于动作空间是连续的，则对于策略函数的参数，Q值函数是可微的**。同时，我们将$$\mathop{max}\limits_{a}Q(s,a)$$用$$Q(s,\mu_\theta)$$来近似替代。

## DDPG中的Q-learning

在Q-learning-based算法中，Q值函数定义为：
$$
Q^*(s,a)=\mathop{E}\limits_{s'\sim P}[r(s,a)+\gamma \mathop{max}\limits_{a'}Q^*(s',a')]
$$
则通过MSBE（Mean-Squared Bellman Error）定义的目标损失函数为：
$$
L(\phi,\mathcal{D})=
\mathop{E}\limits_{(s,a,r,s',d)\sim P}
[(Q_\phi(s,a)-(r+\gamma (1-d)\mathop{max}\limits_{a'}Q_\phi(s',a')))^2]
$$
其中$$d$$为是否为terminal state的标记。DDPG同时利用了策略函数和Target&Current Networks结构，通过将$$\mathop{max}\limits_{a}Q(s,a)$$用$$Q(s,\mu_\theta)$$来近似替代，并加入Target Network，我们将上式写为：
$$
L(\phi,\mathcal{D})=
\mathop{E}\limits_{(s,a,r,s',d)\sim P}
[(Q_\phi(s,a)-(r+\gamma (1-d)\mathop{max}\limits_{a'}Q_{target}(s',\mu_{target}(s'))))^2]
$$

> **Replay Buffer**
>
> Replay Buffer是用来存储Transitions的。对于其中的transitions的利用有窍门：如果利用very-most recent数据，则在训练过程中可能造成过拟合；如果利用太多的transitions（batch_size过大），则可能使得learning变得缓慢。

## DDPG中的Policy Gradient

我们需要学习出一个能够给出使得$$Q_\phi(s,a)$$最大的deterministic policy $$\mu_\theta(s)$$，则可简单地利用gradient ascend：
$$
\mathop{max}\limits_{a'}\mathop{E}\limits_{s\sim \mathcal{D}}[Q_\phi(s,\mu_\theta(s))]
$$
Q值函数的参数$$\phi$$此时被视为常数。

## DDPG中的Tricks 

- **Polyak Averaging**

  Target Network中的参数$$\circ\ _{target}$$都在一定训练步后从Current Network中load进来。但在DDPG中，Target Network根据Polyak Averaging在每次Current Network更新时进行更新：
  $$
  \phi_{target}\leftarrow \rho\ \phi_{target}+(1-\rho)\ \phi_{current}
  $$

- **Exploration & Exploitation**

  为使得DDPG的策略函数进行更好的探索，可在training时加入噪声，最初DDPG的论文的作者建议加入与时间互相关的OU噪声。但最近的研究者建议加入不与时间互相关的、平均值为零的高斯噪声，并在训练过程中逐渐减少噪声的绝对值。在testing时，不加入噪声。还可以利用以均匀分布从动作空间中随机选取动作以提升exploration。

  